{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d351f9e-d4fc-4c4d-92ff-a0d3c8543d20",
   "metadata": {},
   "source": [
    "This notebook is for preliminary steps of text preprocessing: stopwords removal, lowering case, lemmatization.. <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00c7f70-393c-4e3e-9a26-3809941a16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96463e4a-2b41-48e5-b73b-98017168074a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4027f5c4-48a6-448b-9cb0-d00064ee031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tools(language, stopwords=stopwords):\n",
    "    \"\"\"\n",
    "    Load open tasks in DataFrame from csv file.\n",
    "    Define stopwords and lemmatizer appropriate to given language of data.\n",
    "    \"\"\"\n",
    "    if language == \"german\": \n",
    "        stopwords = stopwords.words('german') \n",
    "        lemmatizer = spacy.load(\"de_core_news_sm\") \n",
    "    elif language == \"english\": \n",
    "        stopwords = stopwords.words('english')\n",
    "        lemmatizer = spacy.load('en_core_web_sm')\n",
    "    else:\n",
    "        raise language + \" language not supported.\" \n",
    "    return stopwords, lemmatizer \n",
    "\n",
    "def preprocess(text, lemmatizer, stopwords, stemmer=stemmer):\n",
    "    if isinstance(text, float):\n",
    "        return \"\"\n",
    "    # lowercase, remove punctuation, tokenize \n",
    "    words = simple_preprocess(text, deacc=True, min_len=1, max_len=50)\n",
    "    # remove stopwords \n",
    "    tokens = [word for word in words if word not in stopwords]\n",
    "    # stemming\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    # lemmatize\n",
    "    stemmed_doc = lemmatizer(\" \".join(stems))\n",
    "    lemmas = [s.lemma_ for s in stemmed_doc]\n",
    "    # lowercase again (apparently stemming capitalizes the output) \n",
    "    lemmas = [lemma.lower() for lemma in lemmas] \n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049c92a-7712-4e40-a842-e278c769f40d",
   "metadata": {},
   "source": [
    "**Tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d3faf9-2a70-44de-931f-05e8c6b6a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords, lemmatizer = load_tools(\"english\")\n",
    "filenames = [\n",
    "    \"open_tasks_EN\",\n",
    "    # \"augmented_BT_EN\", \n",
    "    # \"augmented_RD_EN\", \n",
    "    # \"augmented_RI_EN\", \n",
    "    # \"augmented_RS_EN\", \n",
    "    # \"augmented_SR_EN\", \n",
    "]\n",
    "\n",
    "for filename in filenames: \n",
    "    df = pd.read_csv(\"data/\"+filename+\".csv\") \n",
    "    df[\"description\"] = df[\"description\"].apply(lambda text: preprocess(text, lemmatizer, stopwords))\n",
    "    df[\"word_count\"] = df[\"description\"].apply(lambda s: len(s.split())) \n",
    "    df.to_csv(\"data/preprocessed_\"+filename+\".csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64cf06a8-7a5a-41b2-99b8-bb6b9f9f60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords, lemmatizer = load_tools(\"german\")\n",
    "filenames = [\n",
    "    \"open_tasks_DE\",\n",
    "    # \"augmented_BT_DE\", \n",
    "    # \"augmented_RD_DE\", \n",
    "    # \"augmented_RI_DE\", \n",
    "    # \"augmented_RS_DE\", \n",
    "    # \"augmented_SR_DE\", \n",
    "]\n",
    "\n",
    "for filename in filenames: \n",
    "    df = pd.read_csv(\"data/\"+filename+\".csv\") \n",
    "    df[\"description\"] = df[\"description\"].apply(lambda text: preprocess(text, lemmatizer, stopwords))\n",
    "    df[\"word_count\"] = df[\"description\"].apply(lambda s: len(s.split())) \n",
    "    df.to_csv(\"data/preprocessed_\"+filename+\".csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3cd65-13b4-4ffe-b5d6-bab6d79f0064",
   "metadata": {},
   "source": [
    "**Aspects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa59a335-b34e-4444-85a2-7c7da4d7cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords, lemmatizer = load_tools(\"english\")\n",
    "filename = \"concept_aspects_EN\"\n",
    "\n",
    "df = pd.read_csv(\"data/\"+filename+\".csv\") \n",
    "df[\"description\"] = df[\"description\"].apply(lambda text: preprocess(text, lemmatizer, stopwords))\n",
    "df[\"word_count\"] = df[\"description\"].apply(lambda s: len(s.split())) \n",
    "df.to_csv(\"data/preprocessed_\"+filename+\".csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3d8c79-b61b-4571-bca5-b31b942060b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords, lemmatizer = load_tools(\"german\")\n",
    "filename = \"concept_aspects_DE\"\n",
    "\n",
    "df = pd.read_csv(\"data/\"+filename+\".csv\") \n",
    "df[\"description\"] = df[\"description\"].apply(lambda text: preprocess(text, lemmatizer, stopwords))\n",
    "df[\"word_count\"] = df[\"description\"].apply(lambda s: len(s.split())) \n",
    "df.to_csv(\"data/preprocessed_\"+filename+\".csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c949d58d-df5c-4a79-9e53-f8353771bae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspectId</th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>groupId</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9639</td>\n",
       "      <td>word right order</td>\n",
       "      <td>CONCEPT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9937</td>\n",
       "      <td>good answer orang utan strong social bond</td>\n",
       "      <td>CONCEPT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9633</td>\n",
       "      <td>subject verb congruent</td>\n",
       "      <td>CONCEPT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11401</td>\n",
       "      <td>answer mention keyword look correctstat</td>\n",
       "      <td>CONCEPT</td>\n",
       "      <td>55122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9984</td>\n",
       "      <td>verb conjug expect ten expectedten</td>\n",
       "      <td>CONCEPT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aspectId                                description     type  groupId  \\\n",
       "0      9639                           word right order  CONCEPT      NaN   \n",
       "1      9937  good answer orang utan strong social bond  CONCEPT      NaN   \n",
       "2      9633                     subject verb congruent  CONCEPT      NaN   \n",
       "3     11401    answer mention keyword look correctstat  CONCEPT  55122.0   \n",
       "4      9984         verb conjug expect ten expectedten  CONCEPT      NaN   \n",
       "\n",
       "   categoryId  word_count  \n",
       "0         NaN           3  \n",
       "1         NaN           7  \n",
       "2         NaN           3  \n",
       "3         NaN           5  \n",
       "4         NaN           5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/preprocessed_concept_aspects_EN.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d928fc-9632-4f27-968a-e9aa906dc401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb_venv",
   "language": "python",
   "name": "tb_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
