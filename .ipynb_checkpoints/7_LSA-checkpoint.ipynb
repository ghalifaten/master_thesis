{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5f17c4-1285-4aac-ba89-26602330daa8",
   "metadata": {},
   "source": [
    "This notebook explores and implements Latent Semantic Analysis. <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685c647c-afba-4286-825b-5d5591517376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fcf97f-ea20-4e94-bc10-9776ed7b78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_semantic_analysis(texts, num_topics=5):\n",
    "    # Step 1: Convert texts to TF-IDF matrix\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=1, stop_words=stop_words)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Step 2: Apply Singular Value Decomposition (SVD)\n",
    "    svd = TruncatedSVD(n_components=num_topics)\n",
    "    latent_semantic_analysis = svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Step 3: Normalize the output of SVD\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    latent_semantic_analysis = normalizer.fit_transform(latent_semantic_analysis)\n",
    "    \n",
    "    # Step 4: Print the topics and their most relevant terms\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    for i, topic in enumerate(svd.components_):\n",
    "        top_terms_idx = topic.argsort()[:-6:-1] # Top 5 terms\n",
    "        top_terms = [terms[idx] for idx in top_terms_idx]\n",
    "        print(f\"Topic {i+1}: {' | '.join(top_terms)}\")\n",
    "    \n",
    "    return tfidf_vectorizer, svd, normalizer, latent_semantic_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b81f4e-d350-4507-8f4d-74943dc48408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=1, stop_words=stop_words)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    return tfidf_vectorizer, tfidf_matrix\n",
    "\n",
    "def transform_text(text, tfidf_vectorizer, svd, normalizer):\n",
    "    text_tfidf = tfidf_vectorizer.transform([text])\n",
    "    text_lsa = svd.transform(text_tfidf)\n",
    "    text_lsa_normalized = normalizer.transform(text_lsa)\n",
    "    return text_lsa_normalized\n",
    "\n",
    "def assign_topic_to_text(text_lsa_normalized, topics_lsa_normalized):\n",
    "    similarities = cosine_similarity(text_lsa_normalized, topics_lsa_normalized)\n",
    "    most_similar_topic_index = np.argmax(similarities)\n",
    "    return most_similar_topic_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fb0cf-950d-454c-9fcb-95180164d89a",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7841058a-3bd2-4d8e-8b53-4dfdf25b34c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: like | name | rise | societi | mean\n",
      "Topic 2: go | sentenc | chri | say | luke\n",
      "Topic 3: sentenc | write | carolin | edinburgh | question\n",
      "Topic 4: luke | sherlock | abrihim | cafe | mr\n",
      "Topic 5: italian | french | textbox | languag | sport\n",
      "Topic 6: chri | gun | say | gordi | dylan\n",
      "Topic 7: edinburgh | carolin | move | studi | citi\n",
      "Topic 8: societi | differ | uniti | develop | lose\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/final_tasks_EN.csv\")\n",
    "stop_words = stopwords.words('english')\n",
    "df.dropna(subset=[\"description\"], inplace=True)\n",
    "data = df[\"description\"].to_list() \n",
    "num_topics = 8\n",
    "tfidf_vectorizer, svd, normalizer, lsa_output = latent_semantic_analysis(data, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6782c42-37e3-4500-a1fe-3f50dd9e8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"Topic 1: like | name | rise | societi | mean\",\n",
    "    \"Topic 2: go | sentenc | chri | say | luke\", \n",
    "    \"Topic 3: sentenc | write | carolin | edinburgh | question\", \n",
    "    \"Topic 4: luke | sherlock | abrihim | cafe | mr\", \n",
    "    \"Topic 5: italian | french | textbox | languag | sport\", \n",
    "    \"Topic 6: chri | gun | say | gordi | dylan\", \n",
    "    \"Topic 7: edinburgh | carolin | move | studi | citi\", \n",
    "    \"Topic 8: societi | differ | uniti | develop | lose\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df833c36-f77b-4564-bcab-8eda268f5ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'describ esperanza relev name turn find two three question partner answer three quot societi minut becom integr whole look eye see whole differ world togeth natur azizah al hibri american philosoph legal scholar peac uniti similar uniti diver comparison concili differ mikhail gorbachev former russian presid unlik drop water lose ident join ocean man lose societi live man life independ bear develop unlik societi alon develop self b r ambedkar former indian minist law justic hous mango street name english name mean hope spanish mean mani letter mean sad mean wait like number nine muddi color mexican record father play sunday morn shave song like sob great grandmoth name mine hor woman bear like chine year hor suppos bad circumst bear femal think chine lie chine like mexican like woman strong great grandmoth would like know wild hor woman wild marri great grandfath throw sack head carri like fanci chandeli way stori snuff never forgav look window whole life way mani woman sit sad elbow wonder make well get sorri thing want esperanza inherit name want inherit place window school say name funni syllabl make tin hurt roof mouth spanish name make soft someth like silver quit thick sister name magdalena uglier mine magdalena least come home becom nenni alway esperanza would like baptiz new name name like real one nobodi see esperanza lisandra maritza zeze x ye someth like zeze x sandra cisnero hous mango street suppos believ chandeli ˌʃændəˈlɪə kronleucht baptiz taufen'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = df.loc[1101, \"description\"]\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50c8772b-64ad-4332-a880-5e16d9edea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: like | name | rise | societi | mean\n"
     ]
    }
   ],
   "source": [
    "# Transform test text to LSA space\n",
    "test_text_lsa_normalized = transform_text(test_text, tfidf_vectorizer, svd, normalizer)\n",
    "\n",
    "# Transform topics to LSA space\n",
    "list_topics_normalized = []\n",
    "for topic in topics:\n",
    "    list_topics_normalized.append(transform_text(topic, tfidf_vectorizer, svd, normalizer)[0].tolist())\n",
    "    topics_lsa_normalized = np.array(list_topics_normalized)\n",
    "\n",
    "# Assign topic to test text\n",
    "most_similar_topic_index = assign_topic_to_text(test_text_lsa_normalized, topics_lsa_normalized)\n",
    "\n",
    "print(f\"{topics[most_similar_topic_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d913a3-4878-4f2e-af0c-074e90ccfe8b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bce47dcb-bb43-4b1c-a824-78b7c4484739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: englisch | horst | schreib | horen | schreiben\n",
      "Topic 2: satz | schreib | prateritum | hideaway | passiv\n",
      "Topic 3: horen | schreiben | englisch | satz | passiv\n",
      "Topic 4: massachusett | institut | technolog | of | infinitiv\n",
      "Topic 5: infinitiv | komma | denk | prateritum | zubird\n",
      "Topic 6: prateritum | hideaway | hideout | hauptsatz | relativsatz\n",
      "Topic 7: frage | indirekt | direkt | zwei | hauptsatz\n",
      "Topic 8: hauptsatz | zwei | satzen | relativsatz | schreib\n",
      "Topic 9: lair | prateritum | frage | schreib | satz\n",
      "Topic 10: frage | indirekt | direkt | schreib | passiv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/final_tasks_DE.csv\")\n",
    "stop_words = stopwords.words('german')\n",
    "df.dropna(subset=[\"description\"], inplace=True)\n",
    "data = df[\"description\"].to_list() \n",
    "num_topics = 10\n",
    "tfidf_vectorizer, svd, normalizer, lsa_output = latent_semantic_analysis(data, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f94d867-31d9-4f83-8235-8e716d403fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"Topic 1: englisch | horst | schreib | horen | schreiben\", \n",
    "    \"Topic 2: satz | schreib | prateritum | hideaway | passiv\", \n",
    "    \"Topic 3: horen | schreiben | englisch | satz | passiv\", \n",
    "    \"Topic 4: massachusett | institut | technolog | of | infinitiv\", \n",
    "    \"Topic 5: infinitiv | komma | denk | prateritum | zubird\", \n",
    "    \"Topic 6: prateritum | hideaway | hideout | hauptsatz | relativsatz\", \n",
    "    \"Topic 7: frage | indirekt | direkt | zwei | hauptsatz\", \n",
    "    \"Topic 8: hauptsatz | zwei | satzen | relativsatz | schreib\", \n",
    "    \"Topic 9: lair | prateritum | frage | satz | schreib\", \n",
    "    \"Topic 10: frage | indirekt | direkt | schreib | passiv\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4332a76-a3fa-40a5-94e1-71770284ab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'schreib satz konjunktiv geld leih konne'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = df.loc[1101, \"description\"]\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fe80557-7f30-47ba-86f9-155111db2208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 4: massachusett | institut | technolog | of | infinitiv\n"
     ]
    }
   ],
   "source": [
    "test_text = \"It's a question of going to Edinburgh in the time of earthquake.\"\n",
    "\n",
    "# Transform test text to LSA space\n",
    "test_text_lsa_normalized = transform_text(test_text, tfidf_vectorizer, svd, normalizer)\n",
    "\n",
    "# Transform topics to LSA space\n",
    "list_topics_normalized = []\n",
    "for topic in topics:\n",
    "    list_topics_normalized.append(transform_text(topic, tfidf_vectorizer, svd, normalizer)[0].tolist())\n",
    "    topics_lsa_normalized = np.array(list_topics_normalized)\n",
    "    \n",
    "# Assign topic to test text\n",
    "most_similar_topic_index = assign_topic_to_text(test_text_lsa_normalized, topics_lsa_normalized)\n",
    "\n",
    "print(f\"{topics[most_similar_topic_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f9cd8-72fe-4383-9e26-80dc5af7389d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cbdaa0-e29c-4e2a-99bf-44c9b91dc16f",
   "metadata": {},
   "source": [
    "> the LSA output is a matrix where each row represents a document and each column represents a topic. Each value indicates the strength of the association between the document and the topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb_venv",
   "language": "python",
   "name": "tb_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
