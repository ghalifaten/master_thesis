


import warnings
warnings.filterwarnings("ignore")


import pandas as pd
import mysql.connector
from datasets import Dataset, DatasetDict


# Load data 
df = pd.read_csv("data/all_augmented_tasks_EN.csv") 
df = df.dropna(subset=["description"])
df.reset_index(inplace=True, drop=True)
# df = df.sample(500)
df.head()








df_taskAspects = pd.read_csv("data/taskAspects.csv") 
df_taskAspects.head()








mapping_df = pd.merge(df[["taskId", "description"]], df_taskAspects[["taskId", "aspectId"]], how="inner", on=["taskId"]) 
mapping_df = mapping_df.groupby(by="taskId")["aspectId"].apply(list).reset_index()
mapping_df.head()


elements = mapping_df["taskId"].to_list()
labels = {}
for index, row in mapping_df.iterrows():
    taskId = row["taskId"]
    labels[taskId] = row["aspectId"]
# labels


# Positive pairs: those that share (a max number of) labels 
# Negative pairs: those that have NO common label 
from itertools import combinations

def extract_pairs(elements, labels):
    positive_pairs = []
    negative_pairs = []
    for e1, e2 in combinations(elements, 2):
        common_labels = set(labels[e1]).intersection(labels[e2])
        if len(common_labels) > 50:
            positive_pairs.append((e1, e2))
        if not common_labels:
            negative_pairs.append((e1, e2))
    return positive_pairs, negative_pairs

positive_pairs, negative_pairs = extract_pairs(elements[:50], labels)

# len(positive_pairs), len(negative_pairs), len(elements)


from itertools import product
_combinations = list(product(positive_pairs, negative_pairs))
len(_combinations)


import random
triplets = {"anchor": [], "positive": [], "negative": []} 
idx = 0
# combinations = random.sample(_combinations, 100000)
for comb in _combinations: 
    anchor_id = comb[0][0] 
    pos_id = comb[0][1] 
    neg_id = comb[1][1]

    anchor = df[df["taskId"] == anchor_id ]["description"].values[0]
    pos = df[df["taskId"] == pos_id ]["description"].values[0]
    neg = df[df["taskId"] == neg_id]["description"].values[0]

    # print(anchor) 
    # print(pos)

    triplets["anchor"].append(anchor)
    triplets["positive"].append(pos)
    triplets["negative"].append(neg)
    


import pickle 

with open('triplets.pkl', 'wb') as f:
    pickle.dump(triplets, f)





import pickle
with open('data/triplets.pkl', 'rb') as f:
    triplets = pickle.load(f) 


# Convert to Dataset 
dataset = Dataset.from_dict(triplets) 


# Make splits: train, test, validation
train_test = dataset.train_test_split(test_size=0.3)
test_val = train_test["test"].train_test_split(test_size=0.33)

# Recreate Dataset with the three splits 
dataset = DatasetDict({
    'train': train_test['train'],
    'test': test_val['train'],
    'validation': test_val['test']
})

# data


len(dataset['train']), len(dataset['test']), len(dataset['validation'])


# dataset["train"][0:5]





from datasets import load_dataset
from sentence_transformers import (
    SentenceTransformer,
    SentenceTransformerTrainer,
    SentenceTransformerTrainingArguments,
    SentenceTransformerModelCardData,
)
from sentence_transformers.losses import MultipleNegativesRankingLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator
from sentence_transformers import losses

# 1. Load a model to finetune with 2. (Optional) model card data
model = SentenceTransformer("prajjwal1/bert-tiny")

# 3. Load a dataset to finetune on
train_dataset = dataset["train"]#.select(range(100_000))
eval_dataset = dataset["validation"]
test_dataset = dataset["test"]

# 4. Define a loss function
loss = losses.TripletLoss(model=model)

# 5. (Optional) Specify training arguments
args = SentenceTransformerTrainingArguments(
    # Required parameter:
    output_dir="models/bert-tiny-triplet",
    # Optional training parameters:
    num_train_epochs=1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_ratio=0.1,
    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16
    bf16=False,  # Set to True if you have a GPU that supports BF16
    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch
    # Optional tracking/debugging parameters:
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    logging_steps=500,
    # run_name="bert-tiny-triplet",  # Will be used in W&B if `wandb` is installed
)

# 6. (Optional) Create an evaluator & evaluate the base model
dev_evaluator = TripletEvaluator(
    anchors=eval_dataset["anchor"],
    positives=eval_dataset["positive"],
    negatives=eval_dataset["negative"],
)
dev_evaluator(model)

# 7. Create a trainer & train
trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    loss=loss,
    evaluator=dev_evaluator,
)

trainer.train()


# (Optional) Evaluate the trained model on the test set
test_evaluator = TripletEvaluator(
    anchors=test_dataset["anchor"],
    positives=test_dataset["positive"],
    negatives=test_dataset["negative"]
)
test_evaluator(model)


# # 8. Save the trained model
# model.save_pretrained("models/mpnet-base-all-nli-triplet/final")

# # 9. (Optional) Push it to the Hugging Face Hub
# model.push_to_hub("mpnet-base-all-nli-triplet")


from itertools import product
a = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 4)]
b = [(0, 5), (0, 6), (1, 7), (1, 8), (1, 9)] 
p = product(a, b)


for i in p: 
    print(i)


l = list(product(a, b))
[1,2] + [3,4]



