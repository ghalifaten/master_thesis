


import warnings
warnings.filterwarnings("ignore")


import mysql.connector
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
import nltk
from nltk import classify
from nltk import textcat


# Connect to Database
cnx = mysql.connector.connect(user='root', password='taskbase',
                              host='127.0.0.1', port='3309', database='lernnavi')





# Load Tasks
query = "SELECT * FROM Tasks"
df_tasks = pd.read_sql(query, cnx)

# Load types of tasks
query = "SELECT * FROM TaskTypes"
df_types = pd.read_sql(query, cnx)
df_types.rename(columns={'id':'typeId', 'name':'type'}, inplace=True)

# Assign types to tasks
df_tasks = pd.merge(df_tasks, df_types, how="inner", on='typeId')

# Select tasks of type "OPEN"
df_open_tasks = df_tasks[df_tasks["type"] == "OPEN"]

# Load Last versions of tasks
query = "SELECT * FROM LatestTaskVersions"
df_latest = pd.read_sql(query, cnx)

# Select last versions of Open tasks
df_latest_open_versions = pd.merge(df_open_tasks, df_latest, how="inner", on=["version", "taskId"])
display(df_latest_open_versions.head(1))
print(f"Number of OPEN tasks, playground included: {len(df_latest_open_versions)}")





df_content = df_latest_open_versions["content"].apply(json.loads).apply(pd.Series)
df_content.rename(columns={"id":"taskId"}, inplace=True)
df_tenants = df_content["tenant"].apply(pd.Series)
df_tenants.rename(columns={"name":"tenant_name"}, inplace=True)

# Nan tenants
df_content = pd.concat([df_content, df_tenants], axis=1)
print("Percentage of tasks with unspecified tenant: {:.2f}%".format(len(df_content[df_content["tenant_name"].isna()]) / len(df_content) * 100))
df_content.dropna(subset=["tenant_name"], inplace=True)

# Playground
df_playground = df_content[df_content["tenant_name"].str.contains('playground', case=False)]
print("Percentage of playground tasks (after removing Nan tenants): {:.2f}%".format(len(df_playground) / len(df_content) * 100))
df_content = df_content.drop(df_playground.index)
df_open_tasks = df_content.copy()


print(f"Number of OPEN tasks after removing Playground and Nan tenants: {len(df_open_tasks)}")





columns_to_keep = ["taskId", "language", "description", "context", "topic"]
df_open_tasks = df_open_tasks[columns_to_keep]
df_open_tasks.dropna(axis=1, how="all")

# Extract topic ID, potentially useful later
df_topics = df_open_tasks["topic"].apply(pd.Series)
df_topics.rename(columns={"id":"topic_id"}, inplace=True)

df = pd.concat([df_open_tasks.drop("topic", axis=1), df_topics[["topic_id"]]], axis=1)

print(f"Final number of tasks after cleaning: {len(df)}")





df["description"] = df.apply(
    lambda row: row["description"]+" "+row["context"] 
    if isinstance(row["context"], str) 
    else row["description"], 
    axis=1)
df.drop(columns=["context"], inplace=True)
df.head(1)





df["description"] = df["description"].apply(
    lambda text: BeautifulSoup(str(text), 'html.parser').get_text(separator=" ", strip=True).strip())

#Remove empty descriptions 
df = df[df["description"] != ""] 
print(f"Final number of tasks with non empty descriptions: {len(df_open_tasks)}")





df.language.unique()


L = len(df)
l = len(df[df["language"].isna()])
nan_perc = l/L
print("Nan:  {:.2f}%".format(nan_perc*100))

l = len(df[df["language"] == "EN"])
print("ENGLISH:  {:.2f}%".format(l/L*100))

l = len(df[df["language"] == "DE"])
print("GERMAN:  {:.2f}%".format(l/L*100))

# Remove FR and IT tasks 
df = df[~df["language"].isin(["FR", "IT"])]








def detect_lang(detector, text):
    try:
        return detector.guess_language(text)
    except:
        return ""
if nan_perc > 0.2:
    detector = textcat.TextCat()
    df_nan = df[df["language"].isna()].copy()
    df = df.dropna(subset=["language"])
    df_nan["language"] = df_nan["description"].apply(lambda text: detect_lang(detector, text))
    print(len(df))
    print(len(df_nan))


# A quick hand check
# pd.set_option('display.max_rows', None )
# df_nan[df_nan['language'] == "deu"] # Seems correct
# df_nan[df_nan['language'] == "eng"] # A lot of italian and french tasks


df = pd.concat([df, df_nan[df_nan['language'] == "deu"]])
len(df)


df.head()


# Word count
df["word_count"] = df["description"].apply(lambda s: len(s.split())) 

df_eng = df[df["language"] == "EN"] 
df_deu = df[df["language"].isin(["deu", "DE"])]  


len(df_eng), len(df_deu)








from sklearn.feature_extraction.text import CountVectorizer


fig, axs = plt.subplots(1, 2, figsize=(20, 6), sharey=True)
fig.suptitle('Distribution of lengths of descriptions by word count') 
for (i, (df, lang)) in enumerate(zip([df_deu, df_eng], ["German", "English"])): 
    ax = axs[i]
    word_count = df["word_count"].to_list()
    ax.hist(word_count, bins=np.arange(max(word_count)+2)-0.5, histtype='step')
    ax.set_yscale("log")
    ax.set_xscale("log")
    ax.set_title(lang)
    ax.set_xlabel("word count")
    if i == 0:
        ax.set_ylabel("frequency")

fig.savefig("figures/description_lengths", bbox_inches="tight") 


print("Percentage of descriptions with less than 5 words:")
print("English: {:.2f}%".format(len(df_eng[df_eng["word_count"] < 5]) / len(df_eng) * 100))
print("German: {:.2f}%".format(len(df_deu[df_deu["word_count"] < 5]) / len(df_deu) * 100))

print("\nPercentage of descriptions with less than 10 words:")
print("English: {:.2f}%".format(len(df_eng[df_eng["word_count"] < 10]) / len(df_eng) * 100))
print("German: {:.2f}%".format(len(df_deu[df_deu["word_count"] < 10]) / len(df_deu) * 100))








df_eng = df_eng[df_eng["word_count"] > 4]
df_eng.to_csv("data/open_tasks_EN.csv", index_label=False)

df_deu = df_deu[df_deu["word_count"] > 4]
df_deu.to_csv("data/open_tasks_DE.csv", index_label=False)


len(df_eng)


len(df_deu)



