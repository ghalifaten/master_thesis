





import pandas as pd 


from googletrans import Translator
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

def detect_and_translate(text, target_language='en'):
    translator = Translator()
    sentences = sent_tokenize(text)
    translated_sentences = []

    for sentence in sentences:
        try:
            detected_language = translator.detect(sentence).lang
            # print(f"Detected language: {detected_language} for sentence: {sentence}")
            translation = translator.translate(sentence, dest=target_language)
            translated_sentences.append(translation.text)
        except Exception as e:
            print(f"Error translating sentence: {sentence}")
            print(e)
            translated_sentences.append(sentence)  # Append the original sentence if translation fails

    return ' '.join(translated_sentences)


lang = "EN"


folder = f"gen_files/{lang}/"
filename = "concept_open_tasks"
df = pd.read_csv(f"{folder}{filename}.csv") 


df["description"] = df["description"].apply(lambda t: detect_and_translate(t, target_language="en"))
df.to_csv(f"{folder}translated_open_tasks.csv", index_label=False)





# Choose how to fix max length: median or mean length
df["sentence_count"] = df["description"].apply(lambda t: len(sent_tokenize(t)))
df.head()


# Plot lengths of descriptions in number of sentences  
import seaborn as sns

sns.set_theme(style='white', 
              font_scale=0.7, 
              rc={'figure.figsize':(15,8)})

sent_count = df.groupby("sentence_count").count()[["taskId"]].rename(columns={"taskId":"count"}).reset_index()

ax = sns.barplot(data=sent_count,
                 x="sentence_count",
                 y="count",
                 )
ax.set_yscale("log")


sent_count["count"].mean(), sent_count["count"].median()


# Function to trim text to a fixed number of sentences
def trim_to_sentences(text, max_sentences=57):
    sentences = sent_tokenize(text)
    trimmed_text = ' '.join(sentences[:max_sentences])
    return trimmed_text

df["description"] = df["description"].apply(trim_to_sentences)


df["sentence_count"] = df["description"].apply(lambda t: len(sent_tokenize(t)))


df.groupby(by="sentence_count").count()


len(df)


df.to_csv(f"{folder}trimmed_open_tasks.csv", index_label=False)





folder = "gen_files/DE/"
filename = "open_tasks_DE"
df = pd.read_csv(f"{folder}{filename}.csv") 


df["description"] = df["description"].apply(lambda t: detect_and_translate(t, target_language="de"))
df.to_csv(f"{folder}translated_open_tasks.csv", index_label=False)


df.head()


# Choose how to fix max length: median or mean length
df["sentence_count"] = df["description"].apply(lambda t: len(sent_tokenize(t)))
df.head()


# Plot lengths of descriptions in number of sentences  
import seaborn as sns

sns.set_theme(style='white', 
              font_scale=0.7, 
              rc={'figure.figsize':(15,8)})

sent_count = df.groupby("sentence_count").count()[["taskId"]].rename(columns={"taskId":"count"}).reset_index()

ax = sns.barplot(data=sent_count,
                 x="sentence_count",
                 y="count",
                 )
ax.set_yscale("log")


sent_count["count"].mean(), sent_count["count"].median()


# Function to trim text to a fixed number of sentences
def trim_to_sentences(text, max_sentences=40):
    sentences = sent_tokenize(text)
    trimmed_text = ' '.join(sentences[:max_sentences])
    return trimmed_text

df["description"] = df["description"].apply(trim_to_sentences)


df["sentence_count"] = df["description"].apply(lambda t: len(sent_tokenize(t)))


df.groupby(by="sentence_count").count()


df.to_csv(f"{folder}trimmed_open_tasks.csv", index_label=False)



