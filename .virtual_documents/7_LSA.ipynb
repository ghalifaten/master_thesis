


import warnings
warnings.filterwarnings("ignore")


import pandas as pd
from gensim.utils import simple_preprocess
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer
import numpy as np
import spacy
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import CoherenceModel
from gensim.corpora import Dictionary



def lsa(texts, num_topics=5):
    # Step 1: Convert texts to TF-IDF matrix
    tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=1, stop_words=stop_words)
    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)
    
    # Step 2: Apply Singular Value Decomposition (SVD)
    svd = TruncatedSVD(n_components=num_topics)
    latent_semantic_analysis = svd.fit_transform(tfidf_matrix)
    
    # Step 3: Normalize the output of SVD
    normalizer = Normalizer(copy=False)
    latent_semantic_analysis = normalizer.fit_transform(latent_semantic_analysis)
    
    # Step 4: Print the topics and their most relevant terms
    terms = tfidf_vectorizer.get_feature_names_out()
    topics = []
    for i, topic in enumerate(svd.components_):
        top_terms_idx = topic.argsort()[:-21:-1] # Top 50 terms
        top_terms = [terms[idx] for idx in top_terms_idx]
        topics.append(top_terms)
    return tfidf_vectorizer, svd, normalizer, latent_semantic_analysis, topics


def preprocess_text(texts):
    tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=1, stop_words=stop_words)
    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)
    return tfidf_vectorizer, tfidf_matrix

def transform_text(text, tfidf_vectorizer, svd, normalizer):
    text_tfidf = tfidf_vectorizer.transform([text])
    text_lsa = svd.transform(text_tfidf)
    text_lsa_normalized = normalizer.transform(text_lsa)
    return text_lsa_normalized

def assign_topic_to_text(text, topics_lsa_normalized):
    # Transform test text to LSA space
    text_lsa_normalized = transform_text(text, tfidf_vectorizer, svd, normalizer)

    similarities = cosine_similarity(text_lsa_normalized, topics_lsa_normalized)
    # print(similarities)
    most_similar_topic_index = np.argmax(similarities)
    return most_similar_topic_index





# Load data 
df = pd.read_csv("gen_files/EN/preprocessed/trimmed_open_tasks.csv")
stop_words = stopwords.words('english')
df = df.dropna(subset=["description"]).reset_index(drop=True)
data = df["description"].to_list() 


# Tokenize documents for Gensim
tokenized_docs = [doc.split() for doc in data]
# Create a Gensim dictionary and corpus
dictionary = Dictionary(tokenized_docs)
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]


coh_scores = []
for num_topics in range(2,11):
    tfidf_vectorizer, svd, normalizer, lsa_output, topics = lsa(data, num_topics=num_topics)
    # Convert top words per topic into the format required by CoherenceModel
    cm_topics = [[dictionary.token2id[word] for word in topic] for topic in topics]
    # Compute Coherence Score using the 'u_mass' coherence measure
    coherence_model = CoherenceModel(topics=cm_topics, texts=tokenized_docs, dictionary=dictionary, coherence='u_mass')
    coherence_score = coherence_model.get_coherence() # mean of coherence scores per topic
    coh_scores.append(coherence_score)
    if coherence_score == max(coh_scores):
        best_n = num_topics
        best_model = (tfidf_vectorizer, svd, normalizer, lsa_output, topics)


best_n





(tfidf_vectorizer, svd, normalizer, lsa_output, topics) = best_model


# Transform topics to LSA space
list_topics_normalized = []
for topic in topics:
    list_topics_normalized.append(transform_text(" ".join(topic), tfidf_vectorizer, svd, normalizer)[0].tolist())
    topics_lsa_normalized = np.array(list_topics_normalized)


# Assign a topic to each task of df 

df_task_topic = df[["taskId","description"]] 
df_task_topic["task_topic"] = df_task_topic["description"].apply(lambda text: assign_topic_to_text(text, topics_lsa_normalized))
# df_task_topic.head()











df = pd.read_csv("gen_files/EN/preprocessed/concept_aspects.csv")
stop_words = stopwords.words('english')
df = df.dropna(subset=["description"]).reset_index(drop=True)
data = df["description"].to_list() 


# Tokenize documents for Gensim
tokenized_docs = [doc.split() for doc in data]
# Create a Gensim dictionary and corpus
dictionary = Dictionary(tokenized_docs)
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]


coh_scores = []
for num_topics in range(2,11):
    tfidf_vectorizer, svd, normalizer, lsa_output, topics = lsa(data, num_topics=num_topics)
    # Convert top words per topic into the format required by CoherenceModel
    cm_topics = [[dictionary.token2id[word] for word in topic] for topic in topics]
    # Compute Coherence Score using the 'u_mass' coherence measure
    coherence_model = CoherenceModel(topics=cm_topics, texts=tokenized_docs, dictionary=dictionary, coherence='u_mass')
    coherence_score = coherence_model.get_coherence() # mean of coherence scores per topic
    coh_scores.append(coherence_score)
    if coherence_score == max(coh_scores):
        best_n = num_topics
        best_model = (tfidf_vectorizer, svd, normalizer, lsa_output, topics)


(tfidf_vectorizer, svd, normalizer, lsa_output, topics) = best_model


len(topics), best_n


# Transform topics to LSA space
list_topics_normalized = []
for topic in topics:
    list_topics_normalized.append(transform_text(" ".join(topic), tfidf_vectorizer, svd, normalizer)[0].tolist())
    topics_lsa_normalized = np.array(list_topics_normalized)


# Assign a topic to each aspect of df 

df_aspect_topic = df[["aspectId","description"]] 
df_aspect_topic["aspect_topic"] = df_aspect_topic["description"].apply(lambda text: assign_topic_to_text(text, topics_lsa_normalized))
df_aspect_topic.head()





df_task_aspects = pd.read_csv("gen_files/EN/taskAspects.csv")
# df_task_aspects.head()


df_task_aspects = pd.merge(df_task_aspects, df_task_topic, on="taskId", how="inner")
df_task_aspects


df_task_aspects = pd.merge(df_task_aspects, df_aspect_topic, on="aspectId", how="inner")
# df_task_aspects


df_map = df_task_aspects[["task_topic", "aspect_topic"]]
df_map


df_map.drop_duplicates().groupby("task_topic")["aspect_topic"].apply(list).reset_index()





df_task_aspects


# For each task, assign the aspect_topics (from the task's aspects), and compute the probabilities of how often would topic i map to aspect_topic j 


d = df_task_aspects.groupby(["task_topic", "aspect_topic"]).count()[["taskId"]].reset_index()


d1 = df_map.groupby("task_topic").count().reset_index()


r = pd.merge(d, d1, on="task_topic", how="left")


r.head()


r["probability"] = r["taskId"] / r["aspect_topic_y"]


r





df = pd.read_csv("data/final_tasks_DE.csv")
stop_words = stopwords.words('german')
df.dropna(subset=["description"], inplace=True)
data = df["description"].to_list() 
num_topics = 10
tfidf_vectorizer, svd, normalizer, lsa_output = lsa(data, num_topics=num_topics)


topics = [
    "Topic 1: englisch | horst | schreib | horen | schreiben", 
    "Topic 2: satz | schreib | prateritum | hideaway | passiv", 
    "Topic 3: horen | schreiben | englisch | satz | passiv", 
    "Topic 4: massachusett | institut | technolog | of | infinitiv", 
    "Topic 5: infinitiv | komma | denk | prateritum | zubird", 
    "Topic 6: prateritum | hideaway | hideout | hauptsatz | relativsatz", 
    "Topic 7: frage | indirekt | direkt | zwei | hauptsatz", 
    "Topic 8: hauptsatz | zwei | satzen | relativsatz | schreib", 
    "Topic 9: lair | prateritum | frage | satz | schreib", 
    "Topic 10: frage | indirekt | direkt | schreib | passiv", 
]


test_text = df.loc[1101, "description"]
test_text


test_text = "It's a question of going to Edinburgh in the time of earthquake."

# Transform test text to LSA space
test_text_lsa_normalized = transform_text(test_text, tfidf_vectorizer, svd, normalizer)

# Transform topics to LSA space
list_topics_normalized = []
for topic in topics:
    list_topics_normalized.append(transform_text(topic, tfidf_vectorizer, svd, normalizer)[0].tolist())
    topics_lsa_normalized = np.array(list_topics_normalized)
    
# Assign topic to test text
most_similar_topic_index = assign_topic_to_text(test_text_lsa_normalized, topics_lsa_normalized)

print(f"{topics[most_similar_topic_index]}")






