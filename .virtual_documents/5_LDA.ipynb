





import pandas as pd
import gensim.corpora as corpora
from gensim.models.ldamodel import LdaModel
from pprint import pprint
import matplotlib.pyplot as plt
from gensim.models import CoherenceModel
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis


def get_corpus(data, min_len=3):
    # Create a Dictionary: a mapping between words and their integer IDs
    id2word = corpora.Dictionary(data)
    
    # Remove tokens of 1 or 2 letters
    del_ids = [k for k,v in id2word.items() if len(v)<min_len]
    id2word.filter_tokens(bad_ids=del_ids)
    
    # Create a corpus: a list of documents represented as a BoW
    corpus = [id2word.doc2bow(text) for text in data]
    
    return id2word, corpus


def get_model(corpus, id2word, num_topics=3, passes=10, decay=0.5, iterations=50):
    coh_scores = []
    lda_model = LdaModel(
        corpus=corpus, 
        id2word=id2word, 
        num_topics=num_topics, 
        distributed=False,
        passes=passes, 
        update_every=1,
        alpha='auto', 
        eta=None, 
        decay=decay,
        eval_every=5,
        iterations=iterations, 
        per_word_topics=True)
    
    coherence_model_lda = CoherenceModel(
        model=lda_model, 
        texts=data, 
        dictionary=id2word, 
        coherence='c_v')
        
    coherence_lda = coherence_model_lda.get_coherence()
    print(f"Coherence score: {coherence_lda}")

    return lda_model, coherence_lda

def plot_coh_score(coh_scores, title, language, save=True): 
    fig, ax = plt.subplots(1, 1)
    ax.plot(range(2, 11), coh_scores, marker='o', linestyle='--')
    ax.title.set_text(title)
    ax.set_ylabel("Coherence score")
    ax.set_xlabel('Number of topics')
    ax.grid(True)
    if save:
        ax.get_figure().savefig("figures/LDA_coh_"+language, bbox_inches="tight")



def get_best_model(corpus, id2word, title, language, plot=False, save_plot=False):
    coh_scores = []
    for num_topics in range(2, 11):
    # for passes in range(10, 100, 10): 
    # for iterations in range(50, 100, 10): 
    # for decay in [0.6, 0.7, 0.8, 0.9, 1]:
        lda_model, coherence_lda = get_model(corpus, 
                                             id2word, 
                                             num_topics=num_topics, 
                                             passes=passes, 
                                             decay=decay, 
                                             iterations=iterations)
    coh_scores.append(coherence_lda)
    if coherence_lda == max(coh_scores):
        best_model = lda_model

    if plot:
        plot_coh_score(coh_scores, title, language, save_plot)

    return best_model 






































folder = "gen_files/EN/"
df = pd.read_csv(f"{folder}preprocessed/all_preprocessed_open_tasks_EN.csv")
df_taskaspects = pd.read_csv(f"{folder}all_taskAspects_EN.csv")

# Keeping only the tasks that have one or more aspects of type CONCEPT
df = pd.merge(df, df_taskaspects, on="taskId", how="inner") 
df.reset_index(drop=True, inplace=True)


len(df.taskId.unique()), len(df_taskaspects.taskId.unique())


_df = df[["taskId", "description", "topic_id"]].drop_duplicates("taskId")
_df = _df.dropna(subset=["description"]).reset_index()
data = _df["description"].str.split().to_list() 


len(data)


id2word, corpus = get_corpus(data)

num_topics = 3 
passes = 20 
decay = 0.9
iterations = 100
lda_model_en, coherence_lda_en = get_model(corpus=corpus,
                                         id2word=id2word,
                                         num_topics=num_topics,
                                         passes=passes,
                                         decay=decay,
                                         iterations=iterations)


lda_model_en.print_topics() 





documents = _df["description"].to_list()

# Infer topic distributions for each document
topic_distributions = lda_model_en.get_document_topics(corpus)

doc_to_topic = {}
for (i, d) in enumerate(topic_distributions): 
    doc_to_topic[i] = {u:v for (u,v) in d} 
    
df1 = pd.DataFrame.from_dict(doc_to_topic, orient='index').sort_index()
# Replace values that are less than 1/3 by NaN 
df1 = df1.mask(df1 < 1/3).reset_index()


df_tasks_topics = pd.concat([_df[["taskId"]], df1], axis=1) 
print(len(_df), len(df1))


df_tasks_topics


df_taskaspects.head()


df_task_to_aspects = df_taskaspects.groupby(by="taskId")["aspectId"].apply(list).reset_index()
df_task_to_aspects.head()


len(df_tasks_topics), len(df_tasks_topics.taskId.unique()), len(df_task_to_aspects)


df_task_topic_aspect = pd.merge(df_tasks_topics, 
                                df_task_to_aspects, 
                                on="taskId", 
                                how="inner").drop(columns=["index"])
df_task_topic_aspect.sample(5)


df_task_topic_aspect[0]


df_topic_0 = df_task_topic_aspect[[0, "aspectId"]].dropna(subset=[0]).reset_index(drop=True)
df_topic_1 = df_task_topic_aspect[[1, "aspectId"]].dropna(subset=[1]).reset_index(drop=True)
df_topic_2 = df_task_topic_aspect[[2, "aspectId"]].dropna(subset=[2]).reset_index(drop=True)


from collections import Counter
import itertools
aspects_0 = df_topic_0["aspectId"].to_list()
aspects_0 = list(itertools.chain.from_iterable(aspects_0))
occurrences_0 = Counter(aspects_0)

aspects_1 = df_topic_1["aspectId"].to_list()
aspects_1 = list(itertools.chain.from_iterable(aspects_1))
occurrences_1 = Counter(aspects_1)

aspects_2 = df_topic_2["aspectId"].to_list()
aspects_2 = list(itertools.chain.from_iterable(aspects_2))
occurrences_2 = Counter(aspects_2)


# define threshold t as a function of the values of the occurrences.
# defining t as a fixed value risks of having an empty dict result. 
from statistics import median 
t = median(occurrences_0.values())
retained_occ = dict(filter(lambda x: x[1] > t, occurrences_0.items()))
retained_aspects_0 = list(retained_occ.keys())
print(len(retained_aspects_0))

t = median(occurrences_1.values())
retained_occ = dict(filter(lambda x: x[1] > t, occurrences_1.items()))
retained_aspects_1 = list(retained_occ.keys())
print(len(retained_aspects_1))

t = median(occurrences_2.values())
retained_occ = dict(filter(lambda x: x[1] > t, occurrences_2.items()))
retained_aspects_2 = list(retained_occ.keys())
print(len(retained_aspects_2))


# Verify the number of common elements between the aspects of each topic 
# to assess their disparity. 
print(len(set(retained_aspects_0).intersection(set(retained_aspects_1))))
print(len(set(retained_aspects_2).intersection(set(retained_aspects_1))))
print(len(set(retained_aspects_0).intersection(set(retained_aspects_2))))





tasks_topic0 = df_tasks_topics[["taskId", 0]].dropna(subset=[0]).reset_index(drop=True)
tasks_topic0 = pd.merge(tasks_topic0, _df, on="taskId", how="inner").drop(columns=[0,"index"])

data = tasks_topic0["description"].str.split().to_list() 
id2word, corpus = get_corpus(data)

num_topics = 3 
passes = 20 
decay = 0.9
iterations = 100
lda_model_en, coherence_lda_en = get_model(corpus=corpus,
                                         id2word=id2word,
                                         num_topics=num_topics,
                                         passes=passes,
                                         decay=decay,
                                         iterations=iterations)


pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model_en, corpus, id2word) 
vis








d = pd.read_csv("data/taskAspects_EN.csv")
len(d.aspectId.unique())


i = pd.read_csv("data/preprocessed_concept_aspects_EN.csv")
i


df = pd.read_csv("data/preprocessed_concept_aspects_EN.csv")
df.dropna(subset=["description"], inplace=True)
data = df["description"].str.split().to_list() 
title = "Coherence score by number of topics in english tasks" 

id2word, corpus = get_corpus(data)

num_topics = 3 
passes = 20 
decay = 0.9
iterations = 100
lda_model_aspects, coherence_lda_aspects = get_model(corpus=corpus,
                                                     id2word=id2word,
                                                     title=title,
                                                     num_topics=num_topics,
                                                     passes=passes,
                                                     decay=decay,
                                                     iterations=iterations)


lda_model_aspects.print_topics() 


# Visualization 
pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model_aspects, corpus, id2word) 
vis















