{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "685c647c-afba-4286-825b-5d5591517376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad6fadac-5e4f-4117-8a6f-96122f7eba4c",
   "metadata": {},
   "source": [
    "df = pd.read_csv(\"data/open_tasks_EN.csv\")\n",
    "stop_words = stopwords.words('english') + stopwords.words('german')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "num_topics = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c7883fc-0603-4e3e-9bb3-555c7f79e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text): \n",
    " # lowercase, tokenize, and remove stopwords\n",
    "    doc = nlp(text)\n",
    "    verbs = [token.text.lower() for token in doc if token.pos_ == \"VERB\"]\n",
    "    preprocessed_text = simple_preprocess(text)\n",
    "    # words_list = [word for word in preprocessed_text if word not in verbs and word not in stop_words]\n",
    "    words_list = [word for word in preprocessed_text if word not in stop_words]\n",
    "    return \" \".join(words_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "968283e2-39c4-4941-a8bc-e15b44fdc5b5",
   "metadata": {},
   "source": [
    "data = df[\"description\"]\n",
    "preprocessed_data = data.apply(preprocess_data).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d438d4aa-4efd-4472-9084-fa28d6a66bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/preprocessed_open_tasks_EN.csv\")\n",
    "stop_words = stopwords.words('english')\n",
    "data = df[\"preprocess_desc\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad38b548-02d7-42e3-8bae-2fa2949e9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/preprocessed_open_tasks_DE.csv\")\n",
    "# stop_words = stopwords.words('german')\n",
    "# data = df[\"preprocess_desc\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7fcf97f-ea20-4e94-bc10-9776ed7b78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_semantic_analysis(texts, num_topics=5):\n",
    "    # Step 1: Convert texts to TF-IDF matrix\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=1, stop_words=stop_words)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Step 2: Apply Singular Value Decomposition (SVD)\n",
    "    svd = TruncatedSVD(n_components=num_topics)\n",
    "    latent_semantic_analysis = svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Step 3: Normalize the output of SVD\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    latent_semantic_analysis = normalizer.fit_transform(latent_semantic_analysis)\n",
    "    \n",
    "    # Step 4: Print the topics and their most relevant terms\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    for i, topic in enumerate(svd.components_):\n",
    "        top_terms_idx = topic.argsort()[:-6:-1] # Top 5 terms\n",
    "        top_terms = [terms[idx] for idx in top_terms_idx]\n",
    "        print(f\"Topic {i+1}: {' | '.join(top_terms)}\")\n",
    "    \n",
    "    return tfidf_vectorizer, svd, normalizer, latent_semantic_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7841058a-3bd2-4d8e-8b53-4dfdf25b34c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: sentenc | write | anim | say | question\n",
      "Topic 2: italian | textbox | french | sport | languag\n",
      "Topic 3: describ | interact | girl | pictur | dog\n",
      "Topic 4: anim | say | write | interact | dog\n",
      "Topic 5: edinburgh | carolin | move | question | full\n",
      "Topic 6: one | earthquak | build | behav | write\n"
     ]
    }
   ],
   "source": [
    "num_topics = 6\n",
    "tfidf_vectorizer, svd, normalizer, lsa_output = latent_semantic_analysis(data, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cbdaa0-e29c-4e2a-99bf-44c9b91dc16f",
   "metadata": {},
   "source": [
    "> the LSA output is a matrix where each row represents a document and each column represents a topic. Each value indicates the strength of the association between the document and the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec9556b-baa2-4431-80b1-3fd750f3e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = [\n",
    "#     \"Topic 1: grammatik pr채sens passiv satz erg채nze schreibe\",\n",
    "#     \"Topic 2: physik geschwindigkeit nenne elektrischen km auto\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6782c42-37e3-4500-a1fe-3f50dd9e8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"Topic 1: sentenc | write | anim | say | question\", \n",
    "    \"Topic 3: describ | interact | girl | pictur | dog\", \n",
    "    \"Topic 4: anim | say | write | interact | dog\", \n",
    "    \"Topic 5: edinburgh | carolin | move | question | full\", \n",
    "    \"Topic 6: one | earthquak | build | behav | write\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d7c0615-dcd6-4a8b-aca6-ddd2f93de9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"LSA Output:\")\n",
    "# print(lsa_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b0054-5a41-4111-aa5d-afc2d85ea6ad",
   "metadata": {},
   "source": [
    "Assign a given text to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5daf31b5-d978-4327-914c-300c8d3225e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def preprocess_text(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=1, stop_words=stop_words)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    return tfidf_vectorizer, tfidf_matrix\n",
    "\n",
    "def transform_text(text, tfidf_vectorizer, svd, normalizer):\n",
    "    text_tfidf = tfidf_vectorizer.transform([text])\n",
    "    text_lsa = svd.transform(text_tfidf)\n",
    "    text_lsa_normalized = normalizer.transform(text_lsa)\n",
    "    return text_lsa_normalized\n",
    "\n",
    "def assign_topic_to_text(text_lsa_normalized, topics_lsa_normalized):\n",
    "    similarities = cosine_similarity(text_lsa_normalized, topics_lsa_normalized)\n",
    "    most_similar_topic_index = np.argmax(similarities)\n",
    "    return most_similar_topic_index\n",
    "\n",
    "# Example usage:\n",
    "texts = data\n",
    "\n",
    "# Preprocess texts and topics\n",
    "tfidf_vectorizer, tfidf_matrix = preprocess_text(texts)\n",
    "\n",
    "# Perform LSA\n",
    "svd = TruncatedSVD(n_components=len(topics))\n",
    "lsa = svd.fit_transform(tfidf_matrix)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa_normalized = normalizer.fit_transform(lsa)\n",
    "\n",
    "# Transform topics to LSA space\n",
    "list_topics_normalized = []\n",
    "for topic in topics:\n",
    "    list_topics_normalized.append(transform_text(topic, tfidf_vectorizer, svd, normalizer)[0].tolist())\n",
    "    topics_lsa_normalized = np.array(list_topics_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b8b55b-7bdb-4db8-9047-ca914b08085e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 5: edinburgh | carolin | move | question | full\n"
     ]
    }
   ],
   "source": [
    "# Test text\n",
    "# test_text = \"berechnen Sie die Strecke, die das Auto in einer Stunde bei einer Geschwindigkeit von 100 km pro Stunde zur체cklegt.\"\n",
    "test_text = \"It's a question of going to Edinburgh in the time of earthquake.\"\n",
    "\n",
    "# Transform test text to LSA space\n",
    "test_text_lsa_normalized = transform_text(test_text, tfidf_vectorizer, svd, normalizer)\n",
    "\n",
    "# Assign topic to test text\n",
    "most_similar_topic_index = assign_topic_to_text(test_text_lsa_normalized, topics_lsa_normalized)\n",
    "\n",
    "print(f\"{topics[most_similar_topic_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83c33204-c8d5-4fde-baad-15458aabc868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: sentenc | write | anim | say | question\n"
     ]
    }
   ],
   "source": [
    "# Test text\n",
    "# test_text = \"einen Satz im Pr채sens mit dem Verb essen schreiben\"\n",
    "test_text = \"Describe how earthquakes behave\"# in Edinburgh\"\n",
    "\n",
    "# Transform test text to LSA space\n",
    "test_text_lsa_normalized = transform_text(test_text, tfidf_vectorizer, svd, normalizer)\n",
    "\n",
    "# Assign topic to test text\n",
    "most_similar_topic_index = assign_topic_to_text(test_text_lsa_normalized, topics_lsa_normalized)\n",
    "\n",
    "print(f\"{topics[most_similar_topic_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad650ba1-6cbb-4628-bc97-f5a13100d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: sentenc | write | anim | say | question\n"
     ]
    }
   ],
   "source": [
    "# Test text\n",
    "# test_text = \"Write one sentence about how people behave when building after an earthquake\" #Topic 6\n",
    "test_text = \"Write a sentence about how people behave when building after an earthquake\" #Topic 1\n",
    "\n",
    "# Transform test text to LSA space\n",
    "test_text_lsa_normalized = transform_text(test_text, tfidf_vectorizer, svd, normalizer)\n",
    "\n",
    "# Assign topic to test text\n",
    "most_similar_topic_index = assign_topic_to_text(test_text_lsa_normalized, topics_lsa_normalized)\n",
    "\n",
    "print(f\"{topics[most_similar_topic_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cafd341-c69f-4bc7-9800-c813122b946b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb_venv",
   "language": "python",
   "name": "tb_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
